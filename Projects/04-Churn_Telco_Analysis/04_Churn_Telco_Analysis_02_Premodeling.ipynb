{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWjayeNWnJECsZtcTukW1O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leandroaguazaco/data_science_portfolio/blob/main/Projects/04-Churn_Telco_Analysis/04_Churn_Telco_Analysis_02_Premodeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\"> 4 - CHURN TELCO ANALYSIS </h1>\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "  <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/active_project-true-blue\">\n",
        "\n",
        "</div>  \n",
        "\n",
        "<h2 align=\"center\"> 4.1 - Premodeling </h2>\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "  <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/active_section-true-blue\">\n",
        "\n",
        "  <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/section_status-in progress-green\">\n",
        "\n",
        "</div>  \n",
        "\n",
        "<object\n",
        "data=\"https://img.shields.io/badge/contact-Felipe_Leandro_Aguazaco-blue?style=flat&link=https%3A%2F%2Fwww.linkedin.com%2Fin%2Ffelipe-leandro-aguazaco%2F\">\n",
        "</object>"
      ],
      "metadata": {
        "id": "DWwZ5MkbbRKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Project summary\n",
        "\n",
        "The aim of this project is to analyze and predict customer churn in the telco industry. The information pertains to client behavior, including in-call, out-call, and internet service consumption. There is a variable called 'Churn' that determines whether a customer churned within two weeks after canceling services. The information summarizes eight weeks of data for each telco line or client."
      ],
      "metadata": {
        "id": "beWOiXdfb-R1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 align=\"center\"> <font color='orange'>NOTE: The project is distributed across multiple sections, separated into notebook files, in the following way:</font> </h3>\n",
        "\n",
        "\n",
        "\n",
        "4.1 - Preprocessig data: load, join and clean data, and Exploratory data analysis, EDA.\n",
        "\n",
        "> <font color='gray'> 4.2 - Premodeling: predict customer churn based on PyCaret library. </font> ✍ ▶ <font color='orange'> Current section </font>\n",
        "\n",
        "4.3 - Modeling: predict customer churn based on sklearn pipelines.\n",
        "\n",
        "4.4 - Analyzing and explaining predictions.\n",
        "\n",
        "4.5 - Detecting vulneabilities in final machine learnig model.\n",
        "\n",
        "4.6 - Model deployment with Streamlit."
      ],
      "metadata": {
        "id": "0edqHfW8b_Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Install libraries"
      ],
      "metadata": {
        "id": "XqmzfGyudCTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --pre pycaret\n",
        "!pip install rpy2==3.5.1 # Use R\n",
        "!pip install colorama"
      ],
      "metadata": {
        "id": "f3NDdbtVpGdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. Import libraries"
      ],
      "metadata": {
        "id": "u5Ior9hpdk5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# c.1 Python Utilies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import rpy2\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from colorama import Fore, Style"
      ],
      "metadata": {
        "id": "CvEgA7XkdhHv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# c.2 PyCaret\n",
        "from pycaret.classification import *\n",
        "#from pycaret.regression import *"
      ],
      "metadata": {
        "id": "rYgHrs1snqE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c.3 Setups\n",
        "%matplotlib inline\n",
        "plt.style.use(\"ggplot\")\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "metadata": {
        "id": "Hnfg7r_jdzk_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d. Custom functions"
      ],
      "metadata": {
        "id": "yfDCUbTJd6i4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d.1 - Type conversions"
      ],
      "metadata": {
        "id": "MHEVJ9PUeFF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# d.1 dtypes conversion and memory reduce function.\n",
        "def dtype_conversion(df: pd.DataFrame = None, verbose: bool = True)-> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Summary:\n",
        "      Function to dtypes conversion and save reduce memory usage; takes a DataFrame as argument, returns DataFrame.\n",
        "      For more details, visit: https://towardsdatascience.com/how-to-work-with-million-row-datasets-like-a-pro-76fb5c381cdd.\n",
        "      The modifications include type casting for numerical and object variables.\n",
        "    Parameters:\n",
        "      df (pandas.DataFrame): DataFrame containing information.\n",
        "      verbose (bool, default = True): If true, display results (conversions and warnings)\n",
        "    Returns:\n",
        "      pandas.DataFrame: original DataFrame with dtypes conversions\n",
        "      Plot original dtypes status, variable warning due high cardinality, save memory usage, final dtypes status.\n",
        "    \"\"\"\n",
        "    # 0- Original dtypes\n",
        "    # print(Fore.GREEN + \"Input dtypes\" + Style.RESET_ALL)\n",
        "    # print(df.dtypes)\n",
        "    # print(\"\\n\")\n",
        "    print(Fore.RED + \"High Cardinality, categorical features with levels > 15\" + Style.RESET_ALL)\n",
        "\n",
        "    # 1- Original memory_usage in MB\n",
        "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "\n",
        "    # 2- Numerical Types\n",
        "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\": # First 3 characters\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if (c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max):\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                #elif (c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max):\n",
        "                #    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    # 3- Categorical Types\n",
        "    high_card_vars = 0\n",
        "    for col in df.select_dtypes(exclude = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\", \"datetime64[ns]\"]):\n",
        "        categories = list(df[col].unique())\n",
        "        cat_len = len(categories)\n",
        "        if cat_len >= 2 and cat_len < 15:\n",
        "           df[col] = df[col].astype(\"category\")\n",
        "        else:\n",
        "          high_card_vars =+ 1\n",
        "          # Print hight cardinality variables, amount of levels and a sample of 50 firts categories\n",
        "          print(f\"Look at: {Fore.RED + col + Style.RESET_ALL}, {cat_len} levels = {categories[:50]}\")\n",
        "    if high_card_vars == 0:\n",
        "      print(Fore.GREEN + \"None\" + Style.RESET_ALL)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    # 4- Final memory_usage in MB\n",
        "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    if verbose:\n",
        "        print(\"\\n\")\n",
        "        print(f\"{Fore.RED}Initial memory usage: {start_mem:.2f} MB{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.BLUE}Memory usage decreased to {end_mem:.2f} MB ({ 100 * (start_mem - end_mem) / start_mem:.1f}% reduction){Style.RESET_ALL}\")\n",
        "        #print(\"\\n\")\n",
        "        #print(Fore.GREEN + \"Output dtypes\" + Style.RESET_ALL)\n",
        "        #print(df.dtypes)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    # 5. Feature types\n",
        "    print(Fore.GREEN + \"Variable types\" + Style.RESET_ALL)\n",
        "    numerical_vars = len(df.select_dtypes(include = [\"number\"]).columns)\n",
        "    categorical_vars = len(df.select_dtypes(include = [\"category\", \"object\"]).columns)\n",
        "    datetime_vars = len(df.select_dtypes(include = [\"datetime64[ns]\"]).columns)\n",
        "    print(f\"Numerical Features: {numerical_vars}\")\n",
        "    print(f\"Categorical Features: {categorical_vars}\")\n",
        "    print(f\"Datetime Features: {datetime_vars}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "CiwKjBimd_UI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Data"
      ],
      "metadata": {
        "id": "qkOuXDgWeWVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 - Import from Google Drive"
      ],
      "metadata": {
        "id": "3INywgiuedr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Specify the source path in Google Drive\n",
        "drive_filepath = '/content/drive/MyDrive/DataScience_Portfolio/04-Churn_Telco_Analysis/'\n",
        "\n",
        "# Specify the destination path in Colab\n",
        "colab_filepath = '/content/'\n",
        "\n",
        "# Copy the file from Google Drive to Colab\n",
        "try:\n",
        "  shutil.copy(src = drive_filepath + '/churn_data.txt', dst = colab_filepath + '/churn_data.txt')\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efA-KTgjehaf",
        "outputId": "66218ed3-ee78-446a-e484-02752c1406a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 - Load data"
      ],
      "metadata": {
        "id": "uTY9Y_7Ze0Da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "churn_df = pd.read_csv(filepath_or_buffer = \"churn_data.txt\",\n",
        "                       sep = \"|\",\n",
        "                       index_col = \"SUBSCRIBER_ID\",\n",
        "                       parse_dates = True,\n",
        "                       decimal = \".\",\n",
        "                       encoding = \"utf-8\") \\\n",
        "             .assign(region = lambda x: x.loc[:, \"region\"].astype(\"category\")) \\\n",
        "             .pipe(dtype_conversion)\n",
        "\n",
        "# Security copy\n",
        "churn_df_copy = churn_df.copy(deep = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2rcEtjoekRv",
        "outputId": "fb5085a3-c447-498c-a88c-3671066e0641"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mHigh Cardinality, categorical features with levels > 15\u001b[0m\n",
            "Look at: \u001b[31mregion\u001b[0m, 34 levels = ['Bogota D.C.', 'Santander', 'Antioquia', nan, 'Cundinamarca', 'Quindio', 'Valle Del Cauca', 'Arauca', 'Bolivar', 'Atlantico', 'Tolima', 'Huila', 'Meta', 'Putumayo', 'Boyaca', 'Caldas', 'Cordoba', 'Nariño', 'Magdalena', 'Risaralda', 'Cauca', 'La Guajira', 'Norte De Santander', 'Caqueta', 'Cesar', 'Guaviare', 'Sucre', 'Amazonas', 'Guainia', 'Vichada', 'Choco', 'Casanare', 'Providencia Islas', 'Vaupes']\n",
            "\n",
            "\n",
            "\u001b[31mInitial memory usage: 196.19 MB\u001b[0m\n",
            "\u001b[34mMemory usage decreased to 92.12 MB (53.0% reduction)\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[32mVariable types\u001b[0m\n",
            "Numerical Features: 38\n",
            "Categorical Features: 5\n",
            "Datetime Features: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Predicting Customer Churn"
      ],
      "metadata": {
        "id": "a5UrbJ8bfIoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pycaret\n",
        ">[Home - Pycaret](https://pycaret.org/)\n",
        "\n",
        ">[Read the docs - Pycaret](https://pycaret.readthedocs.io/en/stable/index.html)"
      ],
      "metadata": {
        "id": "n3TSOv64fNXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 - Init the experimentation class and init setup"
      ],
      "metadata": {
        "id": "uHf4kCjCfae-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# 2.1 Init the experimentation class\n",
        "churn_binaryclass = ClassificationExperiment()\n",
        "\n",
        "# 2.2 Init setup\n",
        "\n",
        "churn_binaryclass.setup(data = churn_df,\n",
        "                        target = 'churn', # Target column in data\n",
        "                        index = True, # Hold original index\n",
        "                        session_id = 123,\n",
        "                        preprocess = True,\n",
        "                        verbose = True, # When set to False, Information grid is not printed.\n",
        "                        train_size = 0.75, # Proportion of the dataset to be used for training and validation\n",
        "                        test_data = None,\n",
        "\n",
        "                        # Dtypes categorical features\n",
        "                        ordinal_features = None,\n",
        "                        categorical_features = [\"canal\", \"region\", 'bandas', 'tipo_gross_adds'],\n",
        "                        max_encoding_ohe = 30, # Categorical columns with max_encoding_ohe or less unique values are encoded using OneHotEncoding\n",
        "                        rare_to_value = 0.05, # Minimum fraction of category occurrences in a categorical column\n",
        "                        rare_value = \"Otro\", # Value with which to replace rare categories\n",
        "\n",
        "                        # Dtypes numerical features\n",
        "                        numeric_features = churn_df.select_dtypes(include = 'number'),\n",
        "\n",
        "                        # Dtypes datetime features\n",
        "                        date_features = None,\n",
        "                        create_date_columns = [\"day\", \"month\", \"year\"], # Columns to create from the date features\n",
        "\n",
        "                        # Normalize data: depending on similarity of numerical variables scales\n",
        "                        normalize = True,\n",
        "                        normalize_method = \"robust\", # z-score, minmax, maxabs, robust\n",
        "\n",
        "                        # Transform data: depending on skewnees, kurtosis, outiler presence\n",
        "                        transformation = True,\n",
        "                        transformation_method = \"yeo-johnson\",\n",
        "\n",
        "                        # Handled missing values\n",
        "                        imputation_type = \"iterative\",\n",
        "                        iterative_imputation_iters = 5,\n",
        "                        numeric_iterative_imputer = \"rf\", #\"lightgbm\", # Default method\n",
        "                        categorical_iterative_imputer = \"lightgbm\", # Default method\n",
        "\n",
        "                        # Outliers\n",
        "                        remove_outliers = False,\n",
        "                        outliers_method = \"ee\", # \"iforest\": Uses sklearn’s IsolationForest; \"ee\": Uses sklearn’s EllipticEnvelope; \"lof\": Uses sklearn’s LocalOutlierFactor\n",
        "                        outliers_threshold = 0.05, # Percentage of outliers to be removed from the dataset\n",
        "\n",
        "                        # (didn't work)\n",
        "                        # Imbalance data: depending on levels distributions in target variable\n",
        "                        fix_imbalance = True, # Dataset has unequal distribution of target class it can be balanced using this parameter\n",
        "                        fix_imbalance_method = 'SMOTE', # Synthetic Minority Over-sampling Technique, choose from the name of an imblearn estimator\n",
        "\n",
        "                        # (didn't work)\n",
        "                        # Feature selection\n",
        "                        feature_selection = True,\n",
        "                        feature_selection_method = \"univariate\", # \"sequential\": uses sklearn's SequentialFeatureSelector, \"classic\": uses sklearn's SelectFromModel\n",
        "                        feature_selection_estimator = \"rf\", # Classifier used to determine the feature importance\n",
        "                        n_features_to_select = 0.2, # The maximum number of features to select with feature_selection\n",
        "                        low_variance_threshold = 0.1, # Remove features with a training-set variance lower than the provided threshold\n",
        "                        pca = False,\n",
        "\n",
        "                        # Multicollinearity\n",
        "                        remove_multicollinearity = True, # Features with the inter-correlations higher than the defined threshold are removed\n",
        "                        multicollinearity_threshold = 0.85, # Minimum absolute Pearson correlation to identify correlated features\n",
        "                        #remove_perfect_collinearity = True,\n",
        "\n",
        "                        # Cross validation stragery\n",
        "                        data_split_shuffle = True,\n",
        "                        data_split_stratify = True, # Controls stratification during train_test_split\n",
        "                        fold_strategy = \"stratifiedkfold\",\n",
        "                        fold = 5, # Number of folds to be used in cross validation, use in tuning hyperparameters\n",
        "\n",
        "                        # Experiment Logging - Mlflow\n",
        "                        experiment_name = \"Predicting NPS Type - v0.1\",\n",
        "                        log_experiment = True,\n",
        "                        log_plots = True\n",
        ")"
      ],
      "metadata": {
        "id": "neNZgP9Nfh2E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}