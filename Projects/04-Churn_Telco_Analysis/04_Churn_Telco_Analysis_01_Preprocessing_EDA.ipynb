{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM3jKLoB/5Iy+drnEq5LuT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leandroaguazaco/data_science_portfolio/blob/main/Projects/04-Churn_Telco_Analysis/04_Churn_Telco_Analysis_01_Preprocessing_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\"> 4 - CHURN TELCO ANALYSIS </h1>\n",
        "<h2 align=\"center\"> 4.1 - Preprocessing </h2>\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "  <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/active_project-true-blue\">\n",
        "\n",
        "  <img alt=\"Static Badge\" src=\"https://img.shields.io/badge/status-in progress-green\">\n",
        "\n",
        "</div>  \n",
        "\n",
        "<object\n",
        "data=\"https://img.shields.io/badge/contact-Felipe_Leandro_Aguazaco-blue?style=flat&link=https%3A%2F%2Fwww.linkedin.com%2Fin%2Ffelipe-leandro-aguazaco%2F\">\n",
        "</object>"
      ],
      "metadata": {
        "id": "tfDeRLwu88Uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Project summary\n",
        "\n",
        "The aim of this project is to analyze and predict customer churn in the telco industry. The information pertains to client behavior, including in-call, out-call, and internet service consumption. There is a variable called 'Churn' that determines whether a customer churned within two weeks after canceling services. The information summarizes eight weeks of data for each telco line or client."
      ],
      "metadata": {
        "id": "0yX2wde6-RbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 align=\"center\"> <font color='orange'>NOTE: The project is distributed across multiple sections, separated into notebook files, in the following way:</font> </h3>\n",
        "\n",
        "\n",
        "\n",
        "> <font color='gray'> 4.1 - Preprocessig data: load, join and clean data, and Exploratory data analysis, EDA.</font> ✍ ▶ Current section\n",
        "\n",
        "4.2 - Pre-modeling: predict customer churn based on PyCaret library.\n",
        "\n",
        "4.3 - Modeling: predict customer churn based on sklearn pipelines.\n",
        "\n",
        "4.4 - Analyzing and explaining predictions.\n",
        "\n",
        "4.5 - Detecting vulneabilities in final machine learnig model.\n",
        "\n",
        "4.6 - Model deployment with Streamlit."
      ],
      "metadata": {
        "id": "f6v3ks41COqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Install libraries"
      ],
      "metadata": {
        "id": "CFT5PbkhHitW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional libraries such as pandas, numpy, matplotlib, seaborn, and others are already installed in the Colab environment."
      ],
      "metadata": {
        "id": "8JwE66SoHkyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pandas\n",
        "!pip install polars\n",
        "!pip install xlsx2csv\n",
        "!pip install pyjanitor # Clean DataFrame\n",
        "!pip install colorama\n",
        "!pip install adjustText\n",
        "!pip install rpy2==3.5.1 # Use R"
      ],
      "metadata": {
        "id": "BUPbUxi2HqE9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. Import libraries"
      ],
      "metadata": {
        "id": "WHLVvxhfIDxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# c.1 Python Utilies\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import glob\n",
        "import math\n",
        "from scipy.stats import spearmanr\n",
        "import scipy.stats as stats\n",
        "import warnings\n",
        "from janitor import clean_names, remove_empty\n",
        "import rpy2\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# c.2 Visulization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from adjustText import adjust_text\n",
        "from colorama import Fore, Style"
      ],
      "metadata": {
        "id": "MOQKXqzkIHlt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c.3 Setups\n",
        "%matplotlib inline\n",
        "plt.style.use(\"ggplot\")\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "metadata": {
        "id": "tAxrg_MtIies"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d. Custom functions"
      ],
      "metadata": {
        "id": "g6Z2VNB3Ive7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d.1 Load csv files"
      ],
      "metadata": {
        "id": "0tXIjSqkfTTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_readcsv(filepath: str = None) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Summary:\n",
        "    Function to read a csv files, set SUBSCRIBER_ID as index column and drop unncessary column.\n",
        "  Parameters:\n",
        "    file(str, default = None): path to your file of interest.\n",
        "  Return\n",
        "    pandas DataFrame.\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(filepath_or_buffer = filepath,\n",
        "                   sep = \"|\",\n",
        "                   index_col = \"SUBSCRIBER_ID\",\n",
        "                   parse_dates = True,\n",
        "                   decimal = \",\",\n",
        "                   encoding = \"utf-8\") \\\n",
        "         .pipe(lambda x: x.drop([x.columns[0]], axis = 1))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "4qVymP41eG5u"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d.2 Type conversions"
      ],
      "metadata": {
        "id": "MZwaolsnIy9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# d.1 dtypes conversion and memory reduce function.\n",
        "def dtype_conversion(df: pd.DataFrame = None, verbose: bool = True)-> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Summary:\n",
        "      Function to dtypes conversion and save reduce memory usage; takes a DataFrame as argument, returns DataFrame.\n",
        "      For more details, visit: https://towardsdatascience.com/how-to-work-with-million-row-datasets-like-a-pro-76fb5c381cdd.\n",
        "      The modifications include type casting for numerical and object variables.\n",
        "    Parameters:\n",
        "      df (pandas.DataFrame): DataFrame containing information.\n",
        "      verbose (bool, default = True): If true, display results (conversions and warnings)\n",
        "    Returns:\n",
        "      pandas.DataFrame: original DataFrame with dtypes conversions\n",
        "      Plot original dtypes status, variable warning due high cardinality, save memory usage, final dtypes status.\n",
        "    \"\"\"\n",
        "    # 0- Original dtypes\n",
        "    # print(Fore.GREEN + \"Input dtypes\" + Style.RESET_ALL)\n",
        "    # print(df.dtypes)\n",
        "    # print(\"\\n\")\n",
        "    print(Fore.RED + \"High Cardinality, categorical features with levels > 15\" + Style.RESET_ALL)\n",
        "\n",
        "    # 1- Original memory_usage in MB\n",
        "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "\n",
        "    # 2- Numerical Types\n",
        "    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\": # First 3 characters\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if (c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max):\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif (c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max):\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    # 3- Categorical Types\n",
        "    high_card_vars = 0\n",
        "    for col in df.select_dtypes(exclude = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\", \"datetime64[ns]\"]):\n",
        "        categories = list(df[col].unique())\n",
        "        cat_len = len(categories)\n",
        "        if cat_len >= 2 and cat_len < 15:\n",
        "           df[col] = df[col].astype(\"category\")\n",
        "        else:\n",
        "          high_card_vars =+ 1\n",
        "          # Print hight cardinality variables, amount of levels and a sample of 50 firts categories\n",
        "          print(f\"Look at: {Fore.RED + col + Style.RESET_ALL}, {cat_len} levels = {categories[:50]}\")\n",
        "    if high_card_vars == 0:\n",
        "      print(Fore.GREEN + \"None\" + Style.RESET_ALL)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    # 4- Final memory_usage in MB\n",
        "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    if verbose:\n",
        "        print(\"\\n\")\n",
        "        print(f\"{Fore.RED}Initial memory usage: {start_mem:.2f} MB{Style.RESET_ALL}\")\n",
        "        print(f\"{Fore.BLUE}Memory usage decreased to {end_mem:.2f} MB ({ 100 * (start_mem - end_mem) / start_mem:.1f}% reduction){Style.RESET_ALL}\")\n",
        "        #print(\"\\n\")\n",
        "        #print(Fore.GREEN + \"Output dtypes\" + Style.RESET_ALL)\n",
        "        #print(df.dtypes)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    # 5. Feature types\n",
        "    print(Fore.GREEN + \"Variable types\" + Style.RESET_ALL)\n",
        "    numerical_vars = len(df.select_dtypes(include = [\"number\"]).columns)\n",
        "    categorical_vars = len(df.select_dtypes(include = [\"category\", \"object\"]).columns)\n",
        "    datetime_vars = len(df.select_dtypes(include = [\"datetime64[ns]\"]).columns)\n",
        "    print(f\"Numerical Features: {numerical_vars}\")\n",
        "    print(f\"Categorical Features: {categorical_vars}\")\n",
        "    print(f\"Datetime Features: {datetime_vars}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "EMtDcesGI24b"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Load data\n",
        "\n",
        "Four files (.csv):\n",
        "\n",
        "* CONSUMO_DATOS.csv\n",
        "* CONSUMO_VOZ_IN.csv\n",
        "* CONSUMO_VOZ_OUT.csv\n",
        "* INFORMACION_GENERAL.csv"
      ],
      "metadata": {
        "id": "AFZJEjV3KosJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. List of csv.files\n",
        "files = glob.glob('/content/' + '/*.csv')"
      ],
      "metadata": {
        "id": "9T5peHhVPiHB"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. List of DataFrames: four files\n",
        "df_list = [custom_readcsv(file) for file in files]\n",
        "\n",
        "for i in np.arange(0,len(df_list)):\n",
        "  print(f\"Rows = {df_list[i].shape[0]}, Columns = {df_list[i].shape[1]}, File = {files[i][files[i].rfind('/') + 1: ]}\")"
      ],
      "metadata": {
        "id": "f4kbmM51QO1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Concatenate DataFrames\n",
        "churn_data = pd.concat(objs = df_list, axis = 1, join = \"outer\", ignore_index = False, ) \\\n",
        "               .pipe(clean_names)"
      ],
      "metadata": {
        "id": "nFEHojEAfxUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}